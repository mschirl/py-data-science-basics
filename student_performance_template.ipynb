{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vorlage | Student Performance Classification\n",
    "\n",
    "In diesem jupyter notebook werden wir gemeinsam den CRISP-DM Zyklus durcharbeiten, um Ihnen so einen Einblick in die Umsetzung der einzelnen Schritte geben zu können. Als Beispieldatensatz wurde das [Student Performance Data Set](https://archive.ics.uci.edu/ml/datasets/Student+Performance) für sie bereits im Repository im Folder `data/` hinterlegt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inhaltsverzeichnis:**\n",
    "1. [Business Understanding](#bu)\n",
    "1. [Data Understanding](#du)\n",
    "1. [Data Preparation](#dp)\n",
    "1. [Modeling](#md)\n",
    "1. [Evaluation](#ev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. | Business Understanding  <a name=\"bu\"></a>\n",
    "\n",
    "Dieser Teilschritt unseres Projektes beschäftigt sich mit der Definition eigener Projektziele. Außerdem wird unter anderem der IST-Zustand erhoben & ein detaillierter Projektplan erstellt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Das oberste Ziel unseres Workshops lässt sich wie folgt definieren:\n",
    "* Erstellen eines automatisierten Systems zur Evaluierung der Performance von SchülerInnen basierend auf den vorliegenden Daten in den Fächern Portugiesisch & Mathematik.\n",
    "* Unser Modell soll also zwischen jenen SchülerInnen unterscheiden können, die eine positive bzw. negative Abschlussnote bekommen haben.\n",
    "\n",
    "#### Zum Erreichen dieses Zieles müssen folgende Unterziele erfüllt werden:\n",
    "* Explorative Datenanalyse, die uns einen Einblick in die Beschaffenheit der Daten gewährt.\n",
    "* Vorverabeitung der Daten, sodass sie für die Verarbeitung in Machine Learning Algorithmen geeignet sind.\n",
    "* Herausarbeiten eigener Features, die wir als ausschlaggebend für das Abschneiden der SchülerInnen identifizieren (Feature Engineering).\n",
    "* Modellieren einer einfachen Baseline (Support Vector Machine), die uns die Evaluierung unseres (komplexeren) Machine Learning Modells ermöglicht.\n",
    "* Aufbau eines neuronalen Netzes, das für die binäre Klassifizierung (0 - failed, 1 - passed) des SchülerInnenerfolges geeignet ist.\n",
    "* Evaluieren des Trainingsfortschritts (Loss-curves) unseres neuronalen Netzes.\n",
    "* Evaluieren der Performance der binären Klassifizierung mittels `confusion_matrix`, `accuracy_score` & `classification_report`.\n",
    "\n",
    "#### Weitere persönliche Ziele?\n",
    "* ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code | Import statements & vordefinierte Funktionen\n",
    "Dieser Bereich ist für das Hereinladen aller benötigten Module und Funktionen bestehender Bibliotheken, sowie für das Definieren eigener Funktionen gedacht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by importing all necessary modules and functions we need for our project.\n",
    "import os\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm as matplotlib_cm, rc_file_defaults\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input\n",
    "from keras.callbacks import Callback, EarlyStopping, TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may want to use this function when evaluating your model's performance.\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap='Blues'):\n",
    "    \"\"\" Plot Confusion Matrix.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    cm : confusion matrix\n",
    "        may be generated by using sklearn's confusion_matrix function\n",
    "        see https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html for details\n",
    "    classes : array-like of shape (n_classes,)\n",
    "         Target names used for plotting.\n",
    "    normalize : boolean, default=False\n",
    "         Controls normalization of confusion matrix. Set to True if you want to display percentages in your confusion matrix.\n",
    "    cmap : str or matplotlib Colormap, default='Blues'\n",
    "        Colormap recognized by matplotlib.\n",
    "    \"\"\"\n",
    "    \n",
    "    fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        cm = np.around(cm, decimals=3, out=None)  \n",
    "    \n",
    "    \n",
    "    thresh = cm.max() / 2.\n",
    "    \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code adapted from https://stackoverflow.com/questions/51297423/plot-scikit-learn-sklearn-svm-decision-boundary-surface\n",
    "# to help you understand what an SVM does in two dimensions by visualisizing it's decision boundary\n",
    "# important: scale x_feature and y_feature to be within [0, 1] for this function to work properly\n",
    "def plot_svm_decision_boundary(x_feature,\n",
    "                               y_feature,\n",
    "                               y_true,\n",
    "                               clf,\n",
    "                               cmap='Blues',\n",
    "                               x_label='Grade in first Trimester',\n",
    "                               y_label='Grade in second Trimester',\n",
    "                               title='Decision surface of linear SVC',\n",
    "                               scatter_labels=['True Label: Failed', 'True Label: Passed']):\n",
    "    color_map = matplotlib_cm.get_cmap(cmap)\n",
    "    fig, ax = plt.subplots(figsize=(8,8))\n",
    "    xx, yy = make_meshgrid(x_feature, y_feature, h=.0005)\n",
    "    plot_contours(ax, clf, xx, yy, cmap=color_map, alpha=0.75)\n",
    "\n",
    "    #plot negatives:\n",
    "    ax.scatter(x_feature[y_true[y_true==0].index],\n",
    "               y_feature[y_true[y_true==0].index],\n",
    "               color=color_map(0), cmap=color_map, s=25, edgecolors='k', label=scatter_labels[0])\n",
    "    # plot positives:\n",
    "    ax.scatter(x_feature[y_true[y_true==1].index],\n",
    "               y_feature[y_true[y_true==1].index],\n",
    "               color=color_map(255), cmap=color_map, s=25, edgecolors='k', label=scatter_labels[1])\n",
    "\n",
    "    ax.legend()\n",
    "    ax.set_ylabel(y_label)\n",
    "    ax.set_xlabel(x_label)\n",
    "    ax.set_xticks(np.arange(0, 1.1, step=0.1))\n",
    "    ax.set_yticks(np.arange(0, 1.1, step=0.1))\n",
    "    ax.set_title(title)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "#helper functions to declutter plot_svm_decision_boundary \n",
    "def make_meshgrid(x, y, h=.02):\n",
    "    x_min, x_max = 0 - 0.1, 1 + 0.1\n",
    "    y_min, y_max = 0 - 0.1, 1 + 0.1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    return xx, yy\n",
    "\n",
    "def plot_contours(ax, clf, xx, yy, **params):\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    out = ax.contourf(xx, yy, Z, **params)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anlegen der notwendigen Folder für logs & model files die zum Training des neuronalen Netzes notwendig sind\n",
    "\n",
    "logdir = \"logs/\"\n",
    "modeldir = \"models/\"\n",
    "\n",
    "if not os.path.exists(logdir):\n",
    "    os.makedirs(logdir)\n",
    "\n",
    "if not os.path.exists(modeldir):\n",
    "    os.makedirs(modeldir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. | Data Understanding  <a name=\"du\"></a>\n",
    "Üblicherweise beginnt dieser Teil des Zyklus mit dem Sammeln und Beschreiben der Daten, die zu untersuchen sind. Da im Workshop jedoch ein Beispieldatensatz zur Verfügung gestellt wird, können wir uns auf das Wesentliche konzentrieren: das Verstehen der Daten.\n",
    "\n",
    "Als Hilfestellung sei die Quelle des Datensatzes genannt:\n",
    "* P. Cortez and A. Silva. [Using Data Mining to Predict Secondary School Student Performance](http://www3.dsi.uminho.pt/pcortez/student.pdf). In A. Brito and J. Teixeira Eds., Proceedings of 5th FUture BUsiness TEChnology Conference (FUBUTEC 2008) pp. 5-12, Porto, Portugal, April, 2008, EUROSIS, ISBN 978-9077381-39-7.\n",
    "\n",
    "Außerdem folgt die Beschreibung der einzelnen Attribute im der nächsten Zelle dieses jupyter notebooks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Attributes for both student-mat.csv (Math course) and student-por.csv (Portuguese language course) datasets:\n",
    "# 1 school - student's school (binary: \"GP\" - Gabriel Pereira or \"MS\" - Mousinho da Silveira)\n",
    "# 2 sex - student's sex (binary: \"F\" - female or \"M\" - male)\n",
    "# 3 age - student's age (numeric: from 15 to 22)\n",
    "# 4 address - student's home address type (binary: \"U\" - urban or \"R\" - rural)\n",
    "# 5 famsize - family size (binary: \"LE3\" - less or equal to 3 or \"GT3\" - greater than 3)\n",
    "# 6 Pstatus - parent's cohabitation status (binary: \"T\" - living together or \"A\" - apart)\n",
    "# 7 Medu - mother's education (numeric: 0 - none, 1 - primary education (4th grade), 2 – 5th to 9th grade, 3 – secondary education or 4 – higher education)\n",
    "# 8 Fedu - father's education (numeric: 0 - none,  1 - primary education (4th grade), 2 – 5th to 9th grade, 3 – secondary education or 4 – higher education)\n",
    "# 9 Mjob - mother's job (nominal: \"teacher\", \"health\" care related, civil \"services\" (e.g. administrative or police), \"at_home\" or \"other\")\n",
    "# 10 Fjob - father's job (nominal: \"teacher\", \"health\" care related, civil \"services\" (e.g. administrative or police), \"at_home\" or \"other\")\n",
    "# 11 reason - reason to choose this school (nominal: close to \"home\", school \"reputation\", \"course\" preference or \"other\")\n",
    "# 12 guardian - student's guardian (nominal: \"mother\", \"father\" or \"other\")\n",
    "# 13 traveltime - home to school travel time (numeric: 1 - <15 min., 2 - 15 to 30 min., 3 - 30 min. to 1 hour, or 4 - >1 hour)\n",
    "# 14 studytime - weekly study time (numeric: 1 - <2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - >10 hours)\n",
    "# 15 failures - number of past class failures (numeric: n if 1<=n<3, else 4)\n",
    "# 16 schoolsup - extra educational support (binary: yes or no)\n",
    "# 17 famsup - family educational support (binary: yes or no)\n",
    "# 18 paid - extra paid classes within the course subject (Math or Portuguese) (binary: yes or no)\n",
    "# 19 activities - extra-curricular activities (binary: yes or no)\n",
    "# 20 nursery - attended nursery school (binary: yes or no)\n",
    "# 21 higher - wants to take higher education (binary: yes or no)\n",
    "# 22 internet - Internet access at home (binary: yes or no)\n",
    "# 23 romantic - with a romantic relationship (binary: yes or no)\n",
    "# 24 famrel - quality of family relationships (numeric: from 1 - very bad to 5 - excellent)\n",
    "# 25 freetime - free time after school (numeric: from 1 - very low to 5 - very high)\n",
    "# 26 goout - going out with friends (numeric: from 1 - very low to 5 - very high)\n",
    "# 27 Dalc - workday alcohol consumption (numeric: from 1 - very low to 5 - very high)\n",
    "# 28 Walc - weekend alcohol consumption (numeric: from 1 - very low to 5 - very high)\n",
    "# 29 health - current health status (numeric: from 1 - very bad to 5 - very good)\n",
    "# 30 absences - number of school absences (numeric: from 0 to 93)\n",
    "\n",
    "# # these grades are related with the course subject, Math or Portuguese:\n",
    "# 31 G1 - first period grade (numeric: from 0 to 20)  -> corresponds to the first trimester\n",
    "# 31 G2 - second period grade (numeric: from 0 to 20) -> corresponds to the second trimester\n",
    "# 32 G3 - final grade (numeric: from 0 to 20, output target)\n",
    "\n",
    "# Additional note: there are several (382) students that belong to both datasets . \n",
    "# These students can be identified by searching for identical attributes\n",
    "# that characterize each student, as shown in the annexed R file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 | Das Einlesen der Daten \n",
    "Das Einlesen der vorbereiteten CSV-Files in DataFrames erledigen wir über die `read_csv()` Funktion der Bibliothek [pandas](https://pandas.pydata.org/pandas-docs/stable/reference/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads in the CSV file containing grades in subject portuguese.\n",
    "df_por = pd.read_csv('data/student-por.csv', sep=';')\n",
    "# loads in the CSV file containing grades in subject maths.\n",
    "df_mat = pd.read_csv('data/student-mat.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>address</th>\n",
       "      <th>famsize</th>\n",
       "      <th>Pstatus</th>\n",
       "      <th>Medu</th>\n",
       "      <th>Fedu</th>\n",
       "      <th>Mjob</th>\n",
       "      <th>Fjob</th>\n",
       "      <th>...</th>\n",
       "      <th>famrel</th>\n",
       "      <th>freetime</th>\n",
       "      <th>goout</th>\n",
       "      <th>Dalc</th>\n",
       "      <th>Walc</th>\n",
       "      <th>health</th>\n",
       "      <th>absences</th>\n",
       "      <th>G1</th>\n",
       "      <th>G2</th>\n",
       "      <th>G3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>18</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>A</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>at_home</td>\n",
       "      <td>teacher</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>17</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>at_home</td>\n",
       "      <td>other</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>15</td>\n",
       "      <td>U</td>\n",
       "      <td>LE3</td>\n",
       "      <td>T</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>at_home</td>\n",
       "      <td>other</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>15</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>health</td>\n",
       "      <td>services</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>16</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  school sex  age address famsize Pstatus  Medu  Fedu     Mjob      Fjob  ...  \\\n",
       "0     GP   F   18       U     GT3       A     4     4  at_home   teacher  ...   \n",
       "1     GP   F   17       U     GT3       T     1     1  at_home     other  ...   \n",
       "2     GP   F   15       U     LE3       T     1     1  at_home     other  ...   \n",
       "3     GP   F   15       U     GT3       T     4     2   health  services  ...   \n",
       "4     GP   F   16       U     GT3       T     3     3    other     other  ...   \n",
       "\n",
       "  famrel freetime  goout  Dalc  Walc health absences  G1  G2  G3  \n",
       "0      4        3      4     1     1      3        4   0  11  11  \n",
       "1      5        3      3     1     1      3        2   9  11  11  \n",
       "2      4        3      2     2     3      3        6  12  13  12  \n",
       "3      3        2      2     1     1      5        0  14  14  14  \n",
       "4      4        3      2     1     2      5        0  11  13  13  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_por.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>address</th>\n",
       "      <th>famsize</th>\n",
       "      <th>Pstatus</th>\n",
       "      <th>Medu</th>\n",
       "      <th>Fedu</th>\n",
       "      <th>Mjob</th>\n",
       "      <th>Fjob</th>\n",
       "      <th>...</th>\n",
       "      <th>famrel</th>\n",
       "      <th>freetime</th>\n",
       "      <th>goout</th>\n",
       "      <th>Dalc</th>\n",
       "      <th>Walc</th>\n",
       "      <th>health</th>\n",
       "      <th>absences</th>\n",
       "      <th>G1</th>\n",
       "      <th>G2</th>\n",
       "      <th>G3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>18</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>A</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>at_home</td>\n",
       "      <td>teacher</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>17</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>at_home</td>\n",
       "      <td>other</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>15</td>\n",
       "      <td>U</td>\n",
       "      <td>LE3</td>\n",
       "      <td>T</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>at_home</td>\n",
       "      <td>other</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>15</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>health</td>\n",
       "      <td>services</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>16</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  school sex  age address famsize Pstatus  Medu  Fedu     Mjob      Fjob  ...  \\\n",
       "0     GP   F   18       U     GT3       A     4     4  at_home   teacher  ...   \n",
       "1     GP   F   17       U     GT3       T     1     1  at_home     other  ...   \n",
       "2     GP   F   15       U     LE3       T     1     1  at_home     other  ...   \n",
       "3     GP   F   15       U     GT3       T     4     2   health  services  ...   \n",
       "4     GP   F   16       U     GT3       T     3     3    other     other  ...   \n",
       "\n",
       "  famrel freetime  goout  Dalc  Walc health absences  G1  G2  G3  \n",
       "0      4        3      4     1     1      3        6   5   6   6  \n",
       "1      5        3      3     1     1      3        4   5   5   6  \n",
       "2      4        3      2     2     3      3       10   7   8  10  \n",
       "3      3        2      2     1     1      5        2  15  14  15  \n",
       "4      4        3      2     1     2      5        4   6  10  10  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 | Erste Analysen\n",
    "\n",
    "Beantworten Sie folgende Fragen:\n",
    "\n",
    "* Wieviele SchülerInnen sind in den einzelnen Datensätzen aufgezeichnet?\n",
    "* Sind die Datensätze vollständig, oder befinden sich fehlende Werte darin?\n",
    "* Wieviele Spalten enthalten nicht-numerische Werte, die für eine weitere Verarbeitung präprozessiert werden müssen?\n",
    "    * Die Behandlung nicht-numerischer Werte wird nachgelagert in der Phase [Data Preparation](#dp) erledigt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: answer the questions above!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 | Visualisierungen <a name=\"vi\"></a>\n",
    "\n",
    "Histogramme, Boxplots und andere grafische Darstellungsmöglichkeiten sind ein unerlässliches Werkzeug in der Analyse tabellarischer Daten.\n",
    "Da wir mit pandas arbeiten, eignet sich die Bibliothek [seaborn](https://seaborn.pydata.org/index.html) bestens, um solche Visualisierungen zu erstellen.\n",
    "\n",
    "<strong>Disclaimer</strong>: Im für sie bereitgestellten Docker container ist seaborn in der Version 0.10.1 vorinstalliert. Sollten Sie den vollen Umfang der Version 0.11, der auch in der [Dokumentation](https://seaborn.pydata.org/api.html) zu seaborn verwendet wird, nutzen wollen, führen Sie bitte die folgende Zelle im Notebook aus und starten Sie den Kernel des Notebooks neu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install seaborn>=0.11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entwickeln Sie anhand der folgenden Beispiele eigene Analysen im Histogrammformat, indem Sie mit den einzelnen Spalten des Datensatzes experimentieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"darkgrid\")\n",
    "sns.histplot(x=\"age\", hue=\"sex\", data=df_por, multiple='dodge', stat='probability')\n",
    "plt.title('Student\\'s Age distribution by gender')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "sns.histplot(x=\"G3\", hue=\"Dalc\", data=df_por, multiple='dodge', stat='probability', binwidth=5)\n",
    "plt.title('Student\\'s Final Grade distribution in subject Portuguese by weekday alcohol consumption')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,10))\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "sns.boxplot(x=\"school\", y=\"G3\", hue='sex', data=df_por)\n",
    "plt.title('Boxplot of Final Grade in subject Portuguese over Schools for male and female students')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Todo: Analyse the data on your own using the visualisation tools provided!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fühlen Sie sich gerüstet, um in die Phase der Data Preparation überzugehen?\n",
    "\n",
    "Um dies beurteilen zu können, sollten Sie in der Lage sein, folgende Fragen zu beantworten:\n",
    "* Ist die Datenqualität ausreichend, um weiterführende Analysen zu ermöglichen?\n",
    "    * Denken Sie dabei an die Stichworte:\n",
    "        * Vollständigkeit\n",
    "        * Richtigkeit\n",
    "        * Sauberkeit\n",
    "* Können die von uns festgelegten Ziele in der Phase des [Business Understandings](#bu) mit den vorliegenden Daten erreicht werden?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. | Data Preparation  <a name=\"dp\"></a>\n",
    "In dieser Phase werden wir uns um die Vorverarbeitung der Daten kümmern. Typische Schritte enthalten das Auswählen, Säubern, Aggregieren und Formatieren von Daten. Der Schritt des Data Cleanings wurde bereits für uns erledigt, somit ist kein weiteres Säubern der Daten notwendig.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 | Aggregieren\n",
    "\n",
    "Nun, da Sie grundlegend mit den Daten vertraut sind, verbinden wir die beiden Datensätze, um so einen größeren (diverseren) Datensatz für unsere Modelle zu erhalten.\n",
    "* Fügen Sie jedem der beiden DataFrames eine neue Spalte hinzu, der das aufgezeichnete Schulfach enthält.\n",
    "* Kombinieren Sie die beiden DataFrames zu einem einzigen DataFrame `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add a column to your dataframes holding the subject represented within (e.g. 'maths' and 'portuguese')\n",
    "#       combine the two dfs\n",
    "\n",
    "df = df_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 | Formatieren\n",
    "\n",
    "Nach dem Aggregieren der beiden Datensätze, kümmern wir uns nun darum, all jene Spalten zu behandeln, die nicht-numerische Werte beinhalten.\n",
    "\n",
    "* Analysieren sie den DataFrame noch einmal, und geben Sie aus, welche Spalten in numerische Werte zu konvertieren sind.\n",
    "* Überlegen Sie sich eine Strategie zur Codierung kategorischer Werte sowie binärer Werte.\n",
    "    * Beachten Sie dabei auch die Codierung bereits bestehender numerischer Werte!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: find a way to format your non-numerical data.\n",
    "# the pandas Categorical class\n",
    "# see: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Categorical.html\n",
    "# should give you a hint on one (easy) way to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 | Feature Engineering\n",
    "\n",
    "Ein weiterer Schritt der Datenvorverarbeitung ist das Vorselektieren aussagekräftiger Datenmerkmale (engl. Features) bzw.  Spalten, die an die Machine Learning Modellen weitergegeben werden. Im Konkreten ist eine Dimensionsreduktion der bestehenden Daten vorzunehmen. Diese Aufgabe kann basierend auf den Analysen aus Schritt [2.2 | Visualisierungen](#vi) und ihrem Domänenwissen geschehen, oder aber (für die fortgeschritteneren User) auf Basis von Algorithmen wie der [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) übernommen werden.\n",
    "\n",
    "\n",
    "Ein hilfreiches Instrument zur Analyse der Aussagekraft einzelner Features ist die Korrelationsanalyse, also die Analyse des Zusammenhangs der einzelnen Spalten in Ihren Daten. Auch eine spaltenweise Analyse der Daten per Boxplot kann Ihnen möglicherweise zusätzliche Informationen zur Relevanz einzelner Features geben. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the pairwise correlation coefficients of your data set using the DataFrame's corr() Method\n",
    "# see: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html\n",
    "corr = df.corr()\n",
    "sns.set(font_scale=3)\n",
    "plt.figure(figsize=(100,100))\n",
    "sns.heatmap(corr,\n",
    "            xticklabels=corr.columns.values,\n",
    "            yticklabels=corr.columns.values, annot=True, cmap='Blues')\n",
    "sns.despine(offset=10, trim=True)\n",
    "plt.xticks(\n",
    "    rotation=45, \n",
    "    horizontalalignment='right',\n",
    "    fontweight='light' \n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot a boxplot for each column using seaborn.\n",
    "plt.figure(figsize=(40,20))\n",
    "sns.set(font_scale=3)\n",
    "sns.boxplot(data=df)\n",
    "sns.despine(offset=0,\n",
    "            trim=True)\n",
    "plt.xticks(\n",
    "    rotation=45, \n",
    "    horizontalalignment='right',\n",
    "    fontweight='light' \n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typischerweise wird die Aufgabe des Feature Engineerings in Iterationen erledigt, in denen man die Anzahl der gewählten Features schrittweise einschränkt, ein Machine Learning Modell mit den gewählten Features trainiert, um so mehrere Konfigurationen ausprobieren zu können.\n",
    "\n",
    "<strong>Achtung:</strong> Jene Spalte, die von unserem Machine Learning Modell auf Basis der Features vorausgesagt werden soll, darf natürlich <strong>nicht</strong> Teil der Features sein!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: select a subsample of the avalaible columns from the dataframe that you deem to be relevant for the binary classification task\n",
    "# return to this task and after modeling your machine learning models,\n",
    "# and increase/decrease the number of features selected to improve your model's performance\n",
    "# save the subsampled data to a DataFrame (or 2D-Array) 'X'\n",
    "# make sure to exclude column 'G3' from 'X'!\n",
    "\n",
    "X = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abschließend ist nicht zu vergessen, den Zielwert (engl. target oder label) für unsere Machine Learning Modelle festzulegen. Im präsentierten Beispielfall ist eine <strong>binäre Klassifikation</strong> der Gesamtnoten der SchülerInnen vorzunehmen (`df['G3']`). Da die Notenskala jedoch zwischen 0 und 20 definiert ist, ist die Extraktion & Transformation des Zielwerts aus dem Datenset ein Teil der Datenvorverarbeitung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: extract the column 'G3' from the dataset and save it to a 1D array or pandas.Series 'y'\n",
    "# be sure to transform this series from its original values to:\n",
    "#       1 where a student scored G3>=10\n",
    "#       0 everywhere else\n",
    "# This makes sure to prepare or labels for the binary classification task.\n",
    "\n",
    "y = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 | Skalieren\n",
    "\n",
    "Eine weitere typische Aufgabe im Kontext der Datenvorverarbeitung stellt das Skalieren der Daten dar. Diese Skalierung soll dabei helfen, die numerische \"Wichtigkeit\" der Werte einzelner Spalten im Vergleich zu anderen Spalten zu behalten.\n",
    "Auch hier gibt es mehrere Möglichkeiten dies vorzunehmen, eine (spaltenweise) Skalierung aller Werte zwischen \\[0, 1\\] ist ein erster, simpler Ansatz, der hier anzudenken ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: scale your selected features by column to be in the interval [0, 1]\n",
    "# an easy way to do this is using sklearn's MinMaxScaler\n",
    "# see: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#\n",
    "# or you can also do it manually :)\n",
    "y = df['G3']\n",
    "y = np.round(y / max(y)).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. | Modeling  <a name=\"md\"></a>\n",
    "In der Phase des Modellierens werden die nun vorbereiteten Features zum Trainieren von Machine Learning Modellen verwendet. Dazu müssen aber erst einige Voraussetzungen erfüllt sein, bevor das Training beginnen kann. Zur Evaluierung der Modelle ist es notwendig, das bestehende Datenset in drei separate Datensätze aufzuteilen.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 | Training, Testen & Validieren\n",
    "Disclaimer: Die hier nachfolgenden direkten Zitate sind dem Buch [\"Deep Learning\", Goodfellow et al., 2016; Section 5.3](https://www.deeplearningbook.org/contents/ml.html) entnommen.\n",
    "\n",
    "\"We discussed how a held-out test set, composed of examples coming from the same distribution as the training set, can be used to estimate the generalization error of a learner, after the learning process has completed.\"\n",
    "\n",
    "\"It is important that the test examples <strong>are not used in any way to make choices about the model</strong>, including its hyperparameters.\"\n",
    "\n",
    "Worauf uns Goodfellow et al. hiermit hinweisen wollen, ist die Aufteilung unseres Datensatzes in ein <strong>Trainingsset</strong> sowie ein <strong>Testset</strong>.\n",
    "\n",
    "Wichtig dabei ist, dass Trainings-, bzw. Testset jeweils aus Datenpunkten (X) sowie den dazugehörigen Labels (y) bestehen.\n",
    "\n",
    "Das <strong>Trainingsset</strong> wird zum Optimieren der Hyperparameter des Modells verwendet, wohingegen das <strong>Testset</strong> zur Trainingszeit vollkommen unberührt bleibt.\n",
    "\n",
    "Üblicherweise übernimmt die `sklearn` Funktion [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) diese Aufgabe für uns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/train_test_val_split.png\" style=\"width: 50%;\"/></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=None, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aufgrunddessen, dass das Testset vom Trainingsprozess völlig ausgeschlossen wird, wird ein weiteres Datenset benötigt, das zur Evaluierung der Modellperformance während der Trainingszeit verwendet wird.\n",
    "\n",
    "\"In order to create a subset of the data, which will help us tune the hyperparameters of the model, we need to <strong>split the training set into a training and validation set.</strong>\"\n",
    "\n",
    "Wie ein solches Aufteilen des Trainingssets in Trainings-, und Validierungsdaten aussieht, wird beim Trainieren des neuronalen Netzes genauer behandelt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1a | Definition und Evaluierung einer Baseline\n",
    "Vor dem Definieren und Trainieren komplexer Machine Learning Modelle ist es anzuraten, einfachere Ansätze, wie [Naive Bayes Classifier](https://www.cc.gatech.edu/~isbell/reading/papers/Rish.pdf) oder [Support Vector Machines (SVMs)](https://www.kdnuggets.com/2016/07/guyon-data-mining-history-svm-support-vector-machines.html) auf die Problemstellung anzuwenden um eine erste Einschätzung der zu erwartenden Modellperformance zu bekommen.\n",
    "\n",
    "Im gegenständlichen Fall haben wir uns für die Anwendung einer linearen SVM entschieden, die uns in `sklearn` als [LinearSVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html) bereitgestellt wird.\n",
    "Das Ausführen der Methode `fit()` startet dabei bereits den Trainingsprozess der SVM, wobei für ein erfolgreiches Training die Datenpunkte des Trainingssets (X_train) sowie die dazugehörigen Labels (y_train) übergeben werden müssen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = LinearSVC()\n",
    "svm.fit(X_train, y_train, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ohne auf die genaue Funktionalität einer SVM eingehen zu wollen, sei gesagt, dass eine (lineare) SVM vereinfacht gesagt darauf optimiert wird, eine Entscheidungsgrenze (engl. decision boundary) zwischen die Datenpunkte der einzelnen Klassen des Trainingssets zu legen. Eine visuelle Darstellung der Funktionsweise im zweidimensionalen Raum entnehmen Sie bitte folgender Grafik, adaptiert auf Basis dieses [Blogs](http://blog.pengyifan.com/tikz-example-svm-trained-with-samples-from-two-classes/):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/svm.png\" style=\"width: 50%;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die definierte Baseline gilt es nun zu evaluieren, und zwar mit jenen Daten, die aus dem Trainingsprozess herausgehalten worden sind: dem <strong>Testset</strong> bestehend aus den Datenpunkten des Testsets (X_test) sowie den dazugehörigen Labels (y_test).\n",
    "Das Testen beginnt mit dem Aufruf der Methode `predict()` der trainierten SVM (`svm`) welcher X_test übergeben wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Rückgabewert dieser Funktion enthält jene Labels, die der Klassifizierungsalgorithmus für die einzelnen Datenpunkte vorhergesagt hat. Eine Gegenüberstellung der wahren Labels für die Datenpunkte des Testsets (y_test) zu den prädizierten Werten (y_pred), gibt nun Auskunft darüber, wie gut der Klassifizierungsalgorithmus zwischen Datenpunkten einer ungesehen Population unterscheiden gelernt hat.\n",
    "\n",
    "Diese Gegenüberstellung wird üblicherweise durch die Funktionen `confusion_matrix`, `accuracy_score` oder `classification_report` des Packages `sklearn` übernommen. Verwenden Sie auch die eigens hierfür definierte Funktion `plot_confusion_matrix` um die confusion matrix (dt. Wahrheitsmatrix oder Konfusionsmatrix) übersichtlich anzuzeigen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "rc_file_defaults()\n",
    "plot_confusion_matrix(cm,\n",
    "                      classes=['Failed', 'Passed'],\n",
    "                      normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This results in an accuracy score of {:.2f}\".format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach einer ersten Evaluierung ihrer Baseline ist es nun an der Zeit, ihr erstes komplexes Machine Learning Modell zu trainieren!\n",
    "Da ein Deep-Dive in die Funktionsweise neuronaler Netze den heutigen Rahmen sprengen würde, sei uns jedoch zuvor ein kurzer Exkurs in die Darstellung der Entscheidungsgrenze einer SVM erlaubt, die auf das gegenständliche binäre Klassifizierungsproblem optimiert wurde."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1b | Exkurs: Visualisierung der Entscheidungsgrenze einer linearen SVM\n",
    "\n",
    "Obwohl SVMs auch Entscheidungsgrenzen in hochdimensionalen Räumen finden können, reicht die Vorstellungskraft des menschlichen Geistes für solch abstrakte Konstrukte jedoch nicht aus. Etwas Abhilfe soll folgendes Experiment liefern, das ihnen die Funktionalität einer SVM anhand eines Praxisbeispiels näherbringen möchte.\n",
    "\n",
    "Beschränkt man sich in der Wahl der Features auf zwei Dimensionen (im Beispiel wurden die beiden aussagekräftigten Features `'G1'`und `'G2'` gewählt), ist es möglich, die aus dem Trainingsprozess resultierende Entscheidungsgrenze visuell darzustellen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_svm = df[['G1', 'G2']]\n",
    "\n",
    "X_svm = X_svm.apply(lambda column: column/column.max(), axis=0)\n",
    "\n",
    "X_train_svm, X_test_svm, y_train_svm, y_test_svm = train_test_split(X_svm,\n",
    "                                                                    y,\n",
    "                                                                    test_size=0.33,\n",
    "                                                                    random_state=None,\n",
    "                                                                    stratify=y)\n",
    "\n",
    "svm = LinearSVC()\n",
    "svm.fit(X_train_svm, y_train_svm)\n",
    "\n",
    "rc_file_defaults()\n",
    "plot_svm_decision_boundary(x_feature=X_test_svm['G1'],\n",
    "                           y_feature=X_test_svm['G2'],\n",
    "                           y_true=y_test_svm,\n",
    "                           clf=svm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 | Definition eines neuronalen Netzes\n",
    "\n",
    "Im nächsten Teil der Modellierungsphase beschäftigen wir uns mit dem Definieren eines neuronalen Netzes wie es in [Keras](https://keras.io/) vorgenommen wird. Wichtig dabei ist, dass gerade in diesem Schritt technische Details, mathematisches Wissen und Erfahrung in der Optimierung der Netzarchitektur nötig sind, um ein funktionsfähiges neuronales Netz für den gegebenen Anwendungsfall erstellen zu können.\n",
    "\n",
    "Für unseren einfachen Fall der binären Klassifizierung wäre folgende Struktur denkbar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = X_train.shape[1]\n",
    "\n",
    "\n",
    "model = Sequential(name='binary_classification')\n",
    "model.add(Input(shape=(n_features,)))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(2, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "loss_function = \"binary_crossentropy\"\n",
    "optimizer = \"adam\"\n",
    "\n",
    "model.compile(loss=loss_function,\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eine Zusammenfassung der Architektur des Netzes, der darin enthaltenen Schichten und ihrer Hyperparameter gibt die Funktion `model.summary()` aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Starten des Trainingsprozesses beginnt auch in diesem Fall mit dem Aufruf der `fit()` Methode des Modells. Zu beachten ist hierbei, dass im Falle eines neuronalen Netzes nun das weitere Aufteilen des Trainingsset in ein <strong>Trainingsset</strong> und <strong>Validierungsset</strong> für die Evaluierung des Trainingsprozesses zur Laufzeit nötig ist. Diese Aufteilung kann über den Parameter `validation_split` der `fit()` Methode gesteuert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = TensorBoard(log_dir=logdir + \"Binary_classification_students_\" + datetime.now().strftime(\"%Y.%m.%d-%H:%M:%S\"))\n",
    "early_stop = EarlyStopping(monitor='val_loss',\n",
    "                           min_delta=0.005,\n",
    "                           patience=10,\n",
    "                           mode='auto') \n",
    "model.fit(X_train, y_train,\n",
    "        validation_split=0.33,\n",
    "        verbose=1,\n",
    "        callbacks=[early_stop, tb],\n",
    "        epochs=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3a | Evaluieren des Trainingsprozesses\n",
    "\n",
    "Ein wichtiger Teil der Modellierungsphase, die nach dem Training eines Modelles erfolgt, ist die Evaluierung des Trainingsprozesses. Während des Trainings versucht unser neuronales Netz, den entstehenden Fehler bei der binären Klassifizierung der Abschlussnote unserer SchülerInnen zu minimieren. Um zu verhindern, dass dabei allerdings die Trainingsdaten einfach \"auswendig gelernt\" werden, sollte das Training vorzeitig abgebrochen werden, wenn der Fehler gemessen an den Validierungsdaten, größer wird.\n",
    "\n",
    "\n",
    "Eine visuelle Evaluierung des Trainingsprozesses erfolgt meist über das Auswerten sogenannter Lernkurven (engl. learning curves oder loss curves). Diese Kurven zeigen den Verlauf des Fehlers gemessen an Trainings-, und Validierungsdaten über die Trainingsepochen.\n",
    "\n",
    "Sobald Sie ein neuronales Netz trainiert haben, können Sie innerhalb ihres Docker Containers in jenes Verzeichnis wechseln, das den `logs/`Folder enthält (bspw. `/notebooks/py-data-science-basics/`) und dort den Befehl\n",
    "\n",
    "``\n",
    "tensorboard --logdir logs --host 0.0.0.0\n",
    "``\n",
    "\n",
    "starten. Navigieren Sie nun in Ihrem Browser auf http://localhost:6006, um die Lernkurven inspizieren zu können.\n",
    "\n",
    "### 4.3b | Bias & Variance, Overfitting & Underfitting\n",
    "\n",
    "Um die Lernkurven ihres trainierten Modells evaluieren zu können, sollten sie mit den oben genannten Begriffen vertraut sein.\n",
    "Einen einfachen Startpunkt für ihre Recherche zu Bias & Variance bietet dieser Artikel auf [medium.com](https://medium.com/@akgone38/what-the-heck-bias-variance-tradeoff-is-fe4681c0e71b).\n",
    "\n",
    "\n",
    "Mit dem Wissen des Artikels ausgestattet, lassen sich die Begriffe Overfitting & Underfitting anhand der folgenden Grafik erklären:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/overfitting_underfitting.png\" style=\"width: 70%\"/></center>\n",
    "\n",
    "<center><a href=\"https://www.deeplearningbook.org/\">\"Deep Learning\", Goodfellow et al., 2016; Section 5.2</a></center>\n",
    "\n",
    "\n",
    "- \"**Underfitting** occurs when the model is not able to obtain a sufficiently low error value on the training set.\"\n",
    "\n",
    "- \"**Overﬁtting** occurs when the gap between the training error and validation error is too large.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zum Abschluss möchte ich Ihnen Andrew Ng's [Video zum Thema Learning Curves](https://www.youtube.com/watch?v=ISBGFY-gBug) ans Herz legen.\n",
    "\n",
    "Wenn Sie mit dem Trainingsverlauf ihres neuronalen Netzes zufrieden sind, können Sie nun zum letzten Schritt übergehen: Dem Evaluieren ihres neuronalen Netzes auf Basis des <strong>Testdatensets</strong>, dem unberührten Teil der Daten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. | Evaluation  <a name=\"ev\"></a>\n",
    "\n",
    "In der vorläufig letzten Phase unseres Zyklus werden wir nun die Performanz unseres trainierten neuronalen Netzes, ähnlich wie die Evaluierung der SVM, auf die Probe stellen. Hierzu verwenden wir wiederum die Datenpunkte des Testsets (X_test), um die damit `predict()` Methode des trainierten Modells zu füttern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = model.predict(X_test)\n",
    "y_pred = np.round(y_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie Ihnen bereits aufgefallen sein wird, ist eine Nachbearbeitung des Rückgabewerts der `predict()` Methode notwendig. Das neuronale Netz errechnet für jeden einzelnen Datenpunkt im Testset eine Wahrscheinlichkeit, dass dieser Datenpunkt der Klasse 1 zugeordnet werden kann. Ein einfaches Runden der Wahrscheinlichkeiten hilft uns nun bei der Festlegung einer prädizierten Klasse pro Datenpunkt des Testsets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abschließend wenden wir noch einmal die Funktionen `confusion_matrix`, `accuracy_score` und `classification_report` des Packages `sklearn` an, um die wahren Labels (y_test) den prädizierten Labels (y_pred) gegenüberzustellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "class_names = ['Failed', 'Passed']\n",
    "\n",
    "\n",
    "plot_confusion_matrix(cm, class_names,\n",
    "                          normalize=True,\n",
    "                          title='Normalized Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This results in an accuracy score of {:.2f}\".format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abschließende Bemerkungen\n",
    "\n",
    "* Rufen Sie sich noch einmal [Phase 1](#bu) in Erinnerung. \n",
    "* Sind Sie zufrieden mit dem Resultat der binären Klassifizierung der Gesamtnote?\n",
    "* Konnten Sie also auf Anhieb ein neuronales Netz designen, welches bessere Klassifizierungsergebnisse liefert als die SVM?\n",
    "* Sollte dem nicht so sein, wäre es nun an der Zeit, die einzelnen Zyklen des CRISP-DM Modells noch einmal durchzuspielen, besonderes Augenmerk auf die Phasen [Data Understanding](#du) und [Data Preparation](#dp) zu legen, und an ihren Modellen zu schrauben."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
